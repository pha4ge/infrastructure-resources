# Framework for Compute Infrastructure for Pathogen Genomics Labs

**PHA4GE Infrastructure Working Group <br/>**
Authors (order TBD): Daniel Park, Joel Sevinsky, Kelsey Florek, Nabil-Fareed Alikhan, Karin Lagesen, Soyean Kim, Anthony Underwood, Thomas R Connor, Suresh Maslamoney, Kim Lee Ng, Peter van Heusden

Affiliations (order TBD):
1. [PvH] South African Medical Research Council Bioinformatics Unit, South African National Bioinformatics Institute, University of the Western Cape, Bellville, South Africa;
1. [DP] Infectious Disease and Microbiome Program, Broad Institute of MIT and Harvard, MA, USA;
1. [NFA -- update] Quadram Institute Bioscience, Norwich Research Park, Norwich, Norfolk, UK;
1. [JS] Theiagen Genomics, USA;
1. [SK] Faculty of Health Sciences, Simon Fraser University, Canada;
1. [AU –- update] Centre for Genomic Pathogen Surveillance, Big Data Institute, Li Ka Shing Centre for Health Information and Discovery, Nuffield Department of Medicine, University of Oxford, UK;
1. [KF] Wisconsin State Laboratory of Hygiene, Madison, WI, USA;
1. [KL] Norwegian Veterinary Institute, Ås, Norway;
1. [SM] Computational Biology Division, Department of Integrative Biomedical Sciences, Institute of Infectious Disease and Molecular Medicine, University of Cape Town, Cape Town, South Africa
1. [KN] – Bacteria, Parasites and Fungi, Statens Serum Institut, Copenhagen, Denmark

ORCIDs? (good idea but probably depends on journal)
 - NFA: 0000-0002-1243-0767
 - KN: 0000-0002-2679-8845
 - DP: 0000-0001-7226-7781

<details>
 <summary> Document Changelog</summary>
 - 2024-01-30:
    - Move to github MD from gdoc
</details>

## Abstract

With climate change, habitat disruptions, an increase of antibiotic resistance and other anthropogenic and natural factors, combating infections both in humans and animals is again coming to the forefront. Sequencing technology has become an increasingly important part of the toolkit to track and characterise  pathogens, and the use of pathogen genomic data in public health is growing. However,  sequencing technologies have the potential to generate  significant volumes of data, and this requires data management, analysis methods and interpretation tools that might be unfamiliar for the institutions tasked with this work. Here, the PHA4GE Infrastructure working group, consisting of practising bioinformaticians from both academia and healthcare/public health, present a set of recommendations on how institutions can manage this new technology in order to get full use of the data. These recommendations cover aspects including infrastructure, data management, analysis workflow tools and user management. In addition, non-technical considerations, such as legacy systems and regulatory factors are discussed. With these recommendations, the working group aims to provide institutions and working bioinformaticians with a set of best practice guidelines to guide decision making around computational environments used to employ sequencing data to combat disease.

## Background and Motivation

Whole genome sequencing of pathogens has now matured, offering improved methods to public health and clinical microbiology. An increasing number of public health institutions[^Black2020] have adopted genome sequencing as standard practice, and the use of genomics for characterising some pathogens, such as Mycobacterium tuberculosis, is now the gold standard for patient management. Globally, significant large-scale sequencing efforts have bridged the Public Health and Research domains, including for gastrointestinal pathogens (GenomeTrackr) and, most recently, its use over the course of the COVID-19 pandemic[^CDC2020][^COGUK2020][^Ribot2019]. However, the large volumes of data that are generated through the routine use of sequencing technologies, required for analysis, disrupts existing central or enterprise information technology (CIT) infrastructure.  The adoption of genomics within public health organisations  requires an understanding of many of the challenges that have been faced by research computing (Nadon et al. 2017). Interpreting analyses based on genome sequencing (requiring microbiological domain knowledge) and understanding the underlying computing infrastructure requirements (requiring domain knowledge of performance computing and storage) is technically challenging and a skill set that is in need of development in public health institutes. Furthermore, genomic data generated by public health organisations are used to produce analysis results that  inform government policy and regulation. In outbreak or pandemic situations, this analysis needs to happen quickly, while in all circumstances the use of the data demands robustness and data provenance beyond that which is provided by most pre-existing software tools for genome analysis. Moving forward, it is likely that investments in pathogen genomics at a government level will sit alongside investments in clinical human genomics. Therefore, understanding the computational, infrastructure and analysis requirements for pathogen genomic data will be vital in informing the strategic development of genomics-focused infrastructure more generally.

[^Black2020]: Black, Allison, Duncan R. MacCannell, Thomas R. Sibley, and Trevor Bedford. 2020. ‘Ten Recommendations for Supporting Open Pathogen Genomic Analysis in Public Health’. Nature Medicine 26 (6): 832–41. https://doi.org/10.1038/s41591-020-0935-z
[^CDC2020]: CDC 2020. ‘Cases, Data, and Surveillance’. Centers for Disease Control and Prevention. 11 February 2020. https://www.cdc.gov/coronavirus/2019-ncov/variants/spheres.html
[^COGUK2020]: COGUK 2020. ‘An Integrated National Scale SARS-CoV-2 Genomic Surveillance Network’. The Lancet. Microbe 1 (3): e99–100. https://doi.org/10.1016/S2666-5247(20)30054-9
[^Ribot2019]: Ribot, Efrain M., Molly Freeman, Kelley B. Hise, and Peter Gerner-Smidt. 2019. ‘PulseNet: Entering the Age of Next-Generation Sequencing’. Foodborne Pathogens and Disease 16 (7): 451–56. https://doi.org/10.1089/fpd.2019.2634

Here, we present a description of considerations required in designing computing infrastructure that will cater to the requirements of common pathogen genome analysis pipelines. We also present  an assessment of benefits and limitations of existing solutions utilising on premises hardware or cloud-backed computing resources. 

## Methods

Developing computing infrastructure depends on a number of considerations, many of which have been covered elsewhere[^Ahmed2021][^Glatard2017][^Nicholls2021][^Oakeson2017][^Maljkovic2019][^Parkhill2010][^Raza2016] too numerous to address completely. Here we will focus on computing infrastructure as it serves as a platform for running pathogen genomic analyses. We summarise this into four primary questions that pathogen genomics labs must address, and by which we frame our recommendations for compute infrastructure.

[^Ahmed2021]: Ahmed AE, Allen JM, Bhat T, Burra P, Fliege CE, Hart SN, et al. Design considerations for workflow management systems use in production genomics research and the clinic. Sci Rep. 2021;11: 21680. doi:10.1038/s41598-021-99288-8
[^Glatard2017]: Glatard T, Rousseau M-É, Camarasu-Pop S, Adalat R, Beck N, Das S, et al. Software architectures to integrate workflow engines in science gateways. Future Gener Comput Syst. 2017;75: 239–255. doi:10.1016/j.future.2017.01.005
[^Nicholls2021]: Nicholls SM, Poplawski R, Bull MJ, Underwood A, Chapman M, Abu-Dahab K, et al. CLIMB-COVID: continuous integration supporting decentralised sequencing for SARS-CoV-2 genomic surveillance. Genome Biol. 2021;22: 196. doi:10.1186/s13059-021-02395-y
[^Oakeson2017]: Oakeson KF, Wagner JM, Mendenhall M, Rohrwasser A, Atkinson-Dunn R. Bioinformatic Analyses of Whole-Genome Sequence Data in a Public Health Laboratory. Emerg Infect Dis. 2017;23: 1441–1445. doi:10.3201/eid2309.170416
[^Maljkovic2019]: Maljkovic Berry I, Melendrez MC, Bishop-Lilly KA, Rutvisuttinunt W, Pollett S, Talundzic E, et al. Next Generation Sequencing and Bioinformatics Methodologies for Infectious Disease Research and Public Health: Approaches, Applications, and Considerations for Development of Laboratory Capacity. J Infect Dis. 2019. doi:10.1093/infdis/jiz286
[^Parkhill2010]: Parkhill J, Birney E, Kersey P. Genomic information infrastructure after the deluge. Genome Biol. 2010. Available: http://genomebiology.com/content/11/7/402
[^Raza2016]: Raza S, Luheshi L. Big data or bust: realizing the microbial genomics revolution. Microb Genom. 2016;2: e000046. doi:10.1099/mgen.0.000046

  **Where the analysis is run:** The physical infrastructure, hardware components and the degree of abstraction for the user. For instance, analyses may run on a laptop, a server, a high performance computing (HPC) cluster, or remotely on cloud compute resources. This will determine the scale at which the analyses can be run and the degree of abstraction for the user (Figure 1A).

  **How the analysis is run:** The management of analytics workflows or “bioinformatic pipelines.” These might be implemented as a series of command-line scripts or more formal containerised workflow managers (e.g. Snakemake, NextFlow, Workflow Description Language). This will determine the level to which analyses are usable, reusable, portable and reproducible (Figure 1B).

  **How data flows:** Where data comes from, where it is stored during analysis and where it goes. This includes how data is archived and presented (via dashboards, reporting, etc).

  **Who has access:** This encompasses identity and access management, and includes how users are authenticated and authorised to access data. This can be managed by associating users with organisations, projects, and roles. Common defined roles include end users and those with elevated permissions to assist with administration and support.

  **How much does the infrastructure try and solve:** which parts of the pathogen bioinformatics analysis solution do you want to be responsible for? (Figure 2)

**TO DO: insert Figure 1 here**

In many situations, it might be natural to start with defining the infrastructure that the analyses should be run in. However, there are dependencies between how the analyses are run and the infrastructure that supports those analyses. Certain analyses can be more well adapted to specific setups than others. Thus starting with how the analysis will be performed, and determining the infrastructure that best supports that analysis, can be highly beneficial.

### Where does it run: Physical layers of computing

There are a multitude of options for bioinformatic computing infrastructure deployment. The best solution will depend on the specific constraints imposed on a laboratory by their Central IT, Procurement, and other institutional entities. It is important to be aware of the range of solutions available (and unavailable) to make an informed decision. These solutions can be categorised by their degree of abstraction, i.e. the amount of infrastructure that is managed by the user or is deferred to others (Figure 2). Most solutions will belong to one of three major tiers, with SaaS representing the higher abstraction/deferred tier, and IaaS the lowest abstraction/deferred tier:

 - **Software as a Service (SaaS)** - User brings data. Controlled set of pipelines made available to users. All hardware resources and data are managed by others.
 - **Platform as a Service (PaaS)** - User brings data and pipelines. All hardware resources and data are managed by others.
 - **Infrastructure as a Service (IaaS)** - User brings data, pipelines, operating system around pipeline execution and management software. While IaaS is often associated with cloud-hosted virtual machines, we are using the term here to describe all configurations that are fully configured by the user.

All of these solutions may employ on premises hardware, shared/collaborator hardware, commercially rented cloud hardware, or a mix of all three.

![service-diagram-20211116-dp](https://github.com/pha4ge/infrastructure-resources/assets/8513746/d0c66464-28ba-4467-8f20-b0a39cc65d5c)
**Figure 2:** Irresponsibility diagram - what various solutions solve for you. All infrastructure components (Figure; bottom) need to be addressed. The degree to which responsibility can be deferred to a third party depends on the solution chosen. For example, on the IaaS level, a research group will also need to provide their own solution for orchestrating infrastructure (including software installation, selecting and installing a pipeline executor and installing pipelines), which would be solved via SaaS. Thus, selecting a lower level of abstraction introduces additional flexibility with the burden of additional complexity.

### How does it run: Managing bioinformatic compute workflows

**TO DO: add hyperlinks from gdoc to the below sections of text**

There is a minimum requirement of transparency and portability associated with public health protocols that transfers to genome analysis. In order to meet these requirements genome analysis should be run through containerised bioinformatic pipelines expressed in a bioinformatic workflow language, such as those detailed in (Ahmed et al. 2021) and exemplified by  the “Bioinformatic Solutions for SARS-CoV-2 Genomic Analysis” (‘Bioinformatics Solutions For SARS-CoV-2 Genomic Analysis’ 2022) recommendations from the PHA4GE Pipelines and Visualisations Working Group.

The specific workflow managers and workflow languages are continually evolving, but the resources mentioned above include WDL (Workflow Description Language), CWL (Common Workflow Language), Nextflow, and Galaxy. These languages have varying degrees of adoption and support in APIs and services defined by the Global Alliance for Genomics and Health (GA4GH). Labs may employ other workflow languages, such as Snakemake, Airflow, or Swift—these also allow for reproducible and replicable workflows, but may not be as portable in a genomics pipeline sharing ecosystem.

A pipeline sharing ecosystem is a platform (such as Dockstore, WorkflowHub.Eu or nf-core) that manages finding, versioning and sharing pipelines. Workflow execution software (such as the Nextflow runner) or platforms (such as Galaxy or Terra) can often automate selecting and downloading workflow pipelines from such systems, turning workflows into “apps” that can be easily installed and executed.

Workflow execution software (such as the Nextflow runner) or platforms (such as Galaxy or Terra) can often automate the import and execution of workflow pipelines from such ecosystems. SaaS solutions often employ such pipelines under the hood, but only present a curated set to its users.

Ultimately, the choices made here directly impact the ecosystem of easily available bioinformatic pipelines readily available to end users.

### Managing data flow

Data management involves developing practices for storing, controlling access to, versioning, archiving and sharing data extracted from various points in the data flow. Institutions should document these processes, for example through recording analysis provenance data and collecting data management plans at the time of data ingestion.

Data management processes ideally integrate with authentication and authorization processes to allow control through role based access to certain data and to execute data life cycle such as data retention policy. Tagging and classification of data attributes can be used to classify data by risk category (e.g. Personally Identifiable Information), domain, and types of use (for downstream bioinformatic analysis).

Data flows through the system from raw data (produced by instruments like DNA sequencers) and metadata (e.g. date of sample collection) through analysis workflows to final analysis results. The decision about what data counts as an analysis results is somewhat user or pipeline driven but these can be divided broadly into two categories: results that contribute to the analysis of data (for example BAM files that might be examined to interpret a sequence assembly) and results that are ultimately published or archived and constitute conclusions generated from the data. These final results include both genomic data (e.g. consensus genomes or per sample variant summaries) and also analysis outputs such as charts and phylogenies. Typically, raw data is at least an order of magnitude larger than analysis outputs.

In summary, data flow can be described as passing through stages of ingestion, storage processing and delivery (of results, reports and visualisations). Compute infrastructure planning needs to accommodate this data flow.

### Managing user accounts, identity and access

User identity and access management (IAM) is a key requirement of IT infrastructure which involves managing user authentication, life cycle and the access granted to users (or groups) to data or compute resources as well as monitoring and auditability of user activity. The choices a lab makes, with perhaps its limited domain expertise, around implementation of their compute infrastructure have significant implications that must be considered early on, and in most cases, will require coordination with institutional central IT as well as any external providers that are utilised. In many cases, the processing of pathogen genomic data, and user access considerations, would need to be risk assessed and agreed with organisational information governance.

This work will include agreeing the responsibilities for user and role management, something that is likely to be shared between labs and central IT. From the initial provisioning of new user access, to the management of their roles and access levels, review of access as well as the termination and offboarding—user access management is a continuous responsibility. An example of coordinated user management includes the use of the central active directory for accessing services within restricted environments instead of using an independent user management system for authentication and authorization per resource. If this is the case, the genomic compute infrastructure solution should be able to federate logins to other centrally managed (institutional Active Directory, OAuth, SSO) identity systems to facilitate the shared responsibility. Role and authorization management may also be shared. For example, labs may manage access to resources based on group memberships defined by central IT’s AD or group management system.

Responsibilities for cyber incident management, how one detects and responds to data breaches or abuse of compute resources, need to be defined as well. While typically the domain of central IT, the use of a formalised external compute platform or service may shift the burden to the external parties that maintain these services. It is important, in all cases, that Central IT understand data flows and other digital activities being undertaken by a laboratory undertaking pathogen genomics analysis and processing, to ensure that security is properly joined up.  Many external services (such as Terra or DNAnexus) are routinely audited and provide recognized security and privacy certifications (e.g. FISMA, FedRAMP, NIST-800-53, GDPR/Privacy Shield)—if these are necessary, then this may require the use of such a vendor.

## Results

TO DO: the rest of this doc
